{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "82f5b21f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.distributions import Categorical\n",
    "from torch import optim\n",
    "import math\n",
    "import gym\n",
    "\n",
    "#from actor_critic import PolicyNetwork\n",
    "from torch import optim\n",
    "import dataset\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "994813b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PolicyNetwork(nn.Module):\n",
    "    def __init__(self, state_space, n_actions, device, retain_graph=True):\n",
    "        super(PolicyNetwork, self).__init__()\n",
    "        self.base = nn.Linear(state_space, 1024)\n",
    "        self.base2 = nn.Linear(1024, 256)\n",
    "        self.actions = nn.Linear(256, n_actions)\n",
    "        self.value = nn.Linear(256, 1)\n",
    "        self.rewards = []\n",
    "        self.action_pairs = [] #[(LOG_PROB, CRITIC_VALUE)]\n",
    "        self.retain_graph= retain_graph\n",
    "        self.device =device\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.base(x))\n",
    "        x = F.relu(self.base2(x))\n",
    "        a = F.softmax(self.actions(x), dim=-1)\n",
    "        v = self.value(x)\n",
    "        return a,v\n",
    "\n",
    "    def select_action(self, state):\n",
    "        a, v = self.forward(state)\n",
    "        m = Categorical(a)\n",
    "        action = m.sample()\n",
    "        self.action_pairs.append((m.log_prob(action), v))\n",
    "        return action.item()\n",
    "\n",
    "    def addReward(self, r):\n",
    "        self.rewards.append(r)\n",
    "\n",
    "    def train(self, OPTIM, gamma):\n",
    "\n",
    "        ##CONSTRUCT SAMPLED VALUES##\n",
    "        rewards = []\n",
    "        R = 0\n",
    "        for r in self.rewards[::-1]:\n",
    "            R = r + gamma * R\n",
    "            rewards.insert(0, R)\n",
    "\n",
    "        ##NORMALIZE SAMPLED STATE VALUES##\n",
    "        rewards = torch.tensor(rewards, requires_grad=False)\n",
    "\n",
    "        if rewards.shape[0] > 1:\n",
    "            rewards = (rewards-rewards.mean())/(rewards.std() + 1e-4)\n",
    "\n",
    "        ##GET ACTOR AND CRITIC LOSS##\n",
    "        actor_loss = torch.tensor([0], dtype=torch.float32).to(self.device)\n",
    "        critic_loss = torch.tensor([0], dtype=torch.float32).to(self.device)\n",
    "        for (log_prob, val), R in zip(self.action_pairs, rewards):\n",
    "            advantage = R.item() - val.item()\n",
    "            actor_loss += -log_prob*advantage\n",
    "            critic_loss += F.smooth_l1_loss(val, torch.tensor([[R]]).to(self.device))\n",
    "        total_loss = actor_loss + critic_loss\n",
    "\n",
    "        ##OPTIMIZE##\n",
    "        OPTIM.zero_grad()\n",
    "        total_loss.backward(retain_graph=self.retain_graph)\n",
    "        OPTIM.step()\n",
    "\n",
    "        ##CLEAR MEMORY##\n",
    "        del self.rewards[:]\n",
    "        del self.action_pairs[:]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "45638717",
   "metadata": {},
   "outputs": [],
   "source": [
    "REPEAT = 0\n",
    "TERMINAL = 1\n",
    "class RetraEncBlock(nn.Module):\n",
    "    def __init__(self,device, d_model, nhead, actor_optim, actor_train=False, GAMMA=0.99, max_runtime=3, dim_feedforward=2048, dropout=0.1, batch_first=True):\n",
    "        super(RetraEncBlock, self).__init__()\n",
    "        self.encoder_block = nn.TransformerEncoderLayer(d_model, nhead, dim_feedforward=dim_feedforward, \\\n",
    "                                                        dropout=dropout, batch_first=batch_first)\n",
    "        self.actor_critic = PolicyNetwork(d_model, 2, device)\n",
    "        self.actor_optim = actor_optim(self.actor_critic.parameters(), 3e-4)\n",
    "        self.GAMMA = GAMMA\n",
    "        self.actor_train = actor_train\n",
    "        \n",
    "        self.max_runtime = max_runtime\n",
    "    \n",
    "    def propagateLoss(self, loss):\n",
    "        ##Take the negative of the loss so we minimize the loss\n",
    "        assert(self.actor_train)\n",
    "        self.actor_critic.addReward(loss)\n",
    "        self.actor_critic.train(self.actor_optim, self.GAMMA)\n",
    "        \n",
    "    def forward(self, x, mask, pad_mask):\n",
    "        action = REPEAT\n",
    "        i=0\n",
    "        while action == REPEAT and i < self.max_runtime:\n",
    "            x = self.encoder_block(x, mask, pad_mask)\n",
    "            \n",
    "            \n",
    "            if not self.actor_train:\n",
    "                break\n",
    "                \n",
    "            a = self.actor_critic.select_action(x[:,0])\n",
    "            if a == REPEAT:\n",
    "                self.actor_critic.addReward(0)\n",
    "            \n",
    "                \n",
    "            i+=1\n",
    "        return x\n",
    "            \n",
    "class RetraDecBlock(nn.Module):\n",
    "    def __init__(self,device, d_model, nhead, actor_optim, actor_train=False, GAMMA=0.99, max_runtime=20, dim_feedforward=2048, dropout=0.1, batch_first=True):\n",
    "        super(RetraDecBlock, self).__init__()\n",
    "        self.decoder_block = nn.TransformerDecoderLayer(d_model, nhead, dim_feedforward=dim_feedforward, \\\n",
    "                                                        dropout=dropout, batch_first=batch_first)\n",
    "        self.actor_critic = PolicyNetwork(d_model, 2, device)\n",
    "        self.actor_optim = actor_optim(self.actor_critic.parameters(), 1e-5)\n",
    "        self.GAMMA = GAMMA\n",
    "        self.actor_train = actor_train\n",
    "        \n",
    "        \n",
    "        self.max_runtime = max_runtime\n",
    "        \n",
    "    def propagateLoss(self, loss):\n",
    "        ##Take the negative of the loss so we minimize the loss\n",
    "        assert(self.actor_train)\n",
    "        self.actor_critic.addReward(loss)\n",
    "        self.actor_critic.train(self.actor_optim, self.GAMMA)\n",
    "        \n",
    "    def forward(self, x, mem, mask, pad_mask):\n",
    "        action = REPEAT\n",
    "        i=0\n",
    "        while action == REPEAT and i < self.max_runtime:\n",
    "            x = self.decoder_block(x, mem, tgt_mask=mask, tgt_key_padding_mask=pad_mask)\n",
    "            \n",
    "            \n",
    "            if not self.actor_train:\n",
    "                break\n",
    "                \n",
    "            a = self.actor_critic.select_action(x[:,0])\n",
    "            if a == REPEAT:\n",
    "                self.actor_critic.addReward(0)\n",
    "                \n",
    "            i+=1\n",
    "        return x\n",
    "                       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "7d66dcdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RetraNet(nn.Module):\n",
    "    def __init__(self, device,\n",
    "                    actor_optim,\n",
    "                    max_len,\n",
    "                    num_tokens,\n",
    "                    num_encoders=1,\n",
    "                    num_decoders=1,\n",
    "                    dim=64,\n",
    "                    nhead=8,\n",
    "                    d_feedforward=1024,\n",
    "                    batch_first=True):\n",
    "        super(RetraNet, self).__init__()\n",
    "        self.max_len = max_len\n",
    "        self.device = device\n",
    "        self.tokens = num_tokens\n",
    "        ##Create encoder layers##\n",
    "        self.src_emb = nn.Embedding(num_tokens, dim)\n",
    "        self.tgt_emb = nn.Embedding(num_tokens, dim)\n",
    "        self.pos_emb = nn.Embedding(max_len, dim)\n",
    "        ##Create Transformer##\n",
    "        self.encoders = [RetraEncBlock(device, dim, nhead, actor_optim).to(device) for i in range(num_encoders)]\n",
    "        \n",
    "        self.decoders = [RetraDecBlock(device, dim,nhead, actor_optim).to(device) for i in range(num_decoders)]\n",
    "        ##Create Final Linear Layer##\n",
    "        self.linear = nn.Linear(dim, num_tokens)\n",
    "        ##Create TimeStep Input##\n",
    "        self.timesteps = torch.Tensor([[i for i in range(max_len)]]).type(torch.LongTensor).to(device)\n",
    "    def forward(self, src, tgt, src_mask, tgt_mask, src_pad_mask, tgt_pad_mask):\n",
    "        pos_emb = self.pos_emb(self.timesteps)\n",
    "        src_emb = self.src_emb(src)\n",
    "        tgt_emb = self.tgt_emb(tgt)\n",
    "        src_in = pos_emb + src_emb\n",
    "        tgt_in = pos_emb + tgt_emb\n",
    "        for enc in self.encoders:\n",
    "            mem = enc(src_in, src_mask, src_pad_mask)\n",
    "        for dec in self.decoders:\n",
    "            out = dec(tgt_in, mem, tgt_mask, tgt_pad_mask)\n",
    "        \n",
    "        return (self.linear(out))\n",
    "    \n",
    "    def propagateLoss(self, loss):\n",
    "        for enc in self.encoders:\n",
    "            enc.propagateLoss(loss)\n",
    "        for dec in self.decoders:\n",
    "            dec.propagateLoss(loss)\n",
    "    def toggleActor(self):\n",
    "        for enc in self.encoders:\n",
    "            enc.actor_train = not enc.actor_train\n",
    "        for dec in self.decoders:\n",
    "            dec.actor_train = not dec.actor_train\n",
    "        \n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "c33d9865",
   "metadata": {},
   "outputs": [],
   "source": [
    " device = torch.device('cuda')\n",
    "# m = RetraNet(device, optim.SGD , 10, 50, dim=64).to(device)\n",
    "# x = torch.zeros((1, 10), requires_grad=True).type(torch.LongTensor).to(device)\n",
    "# y = torch.zeros((1, 10), requires_grad=True).type(torch.LongTensor).to(device)\n",
    "# m(x,y, None, None, None, None)\n",
    "# m.propagateLoss(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "f474d140",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_square_subsequent_mask(sz):\n",
    "    mask = (torch.triu(torch.ones((sz, sz), device=d)) == 1).transpose(0, 1)\n",
    "    mask = mask.float().masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0))\n",
    "    return mask\n",
    "\n",
    "\n",
    "def create_mask(src, tgt, dset):\n",
    "    src_seq_len = src.shape[1]\n",
    "    tgt_seq_len = tgt.shape[1]\n",
    "\n",
    "    tgt_mask = generate_square_subsequent_mask(tgt_seq_len)\n",
    "    src_mask = torch.zeros((src_seq_len, src_seq_len),device=d).type(torch.bool)\n",
    "\n",
    "    src_padding_mask = (src == dset.TOKENS[\"<PAD>\"])\n",
    "    tgt_padding_mask = (tgt == dset.TOKENS[\"<PAD>\"])\n",
    "    return src_mask, tgt_mask, src_padding_mask, tgt_padding_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "f0b571b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, optim, crit, device, iterations, dset, batch_size, print_freq, scheduler):\n",
    "    running_loss = 0\n",
    "    for it in range(iterations):\n",
    "        x,y,y_ = dset.get_batch(batch_size)\n",
    "        x = x.type(torch.LongTensor).to(device)\n",
    "        y = y.type(torch.LongTensor).to(device)\n",
    "        y_ = y_.type(torch.LongTensor).to(device)\n",
    "        \n",
    "        src_mask, tgt_mask, src_pad_mask, tgt_pad_mask = create_mask(x,y,dset)\n",
    "        model_out = model(x, y, src_mask, tgt_mask, src_pad_mask, tgt_pad_mask)\n",
    "        \n",
    "        optim.zero_grad()\n",
    "        \n",
    "        loss = crit(model_out.reshape(-1, model_out.shape[-1]), y_.reshape(-1))\n",
    "        if(model.encoders[0].actor_train):\n",
    "            model.propagateLoss(loss.item())\n",
    "        loss.backward()\n",
    "        \n",
    "        optim.step()\n",
    "        running_loss+=loss.item()\n",
    "        scheduler.step()\n",
    "        \n",
    "        if (it+1) % print_freq == 0:\n",
    "            print(\"Iteration:\",it+1,\"Loss:\",running_loss/print_freq)\n",
    "            running_loss=0\n",
    "            \n",
    "def convert(expression:str, dset, model, device):\n",
    "    ##Convert String to a tensor##\n",
    "    src = torch.tensor([dset.tokenize_expression(expression)]).to(device)\n",
    "    src = src[:, 1:]\n",
    "    ##Set up output##\n",
    "    y = torch.ones(1, dset.max_len).fill_(dset.TOKENS[\"<PAD>\"]).type(torch.long).to(device)\n",
    "    y[0,0] = dset.TOKENS[\"<SOS>\"]\n",
    "    print(x.shape)\n",
    "    print(y.shape)\n",
    "    model.eval()\n",
    "    for i in range(dset.max_len-1):\n",
    "        src_mask, tgt_mask, src_pad_mask, tgt_pad_mask = create_mask(src,y,dset)\n",
    "        out = model(src, y, src_mask, tgt_mask, src_pad_mask, tgt_pad_mask)\n",
    "        probs = out[0,i]\n",
    "        next_token = torch.argmax(probs,dim=0)\n",
    "        y[0, i+1] = next_token\n",
    "        if next_token == dset.TOKENS[\"<EOS>\"]:\n",
    "            break\n",
    "    y = y.squeeze(0)\n",
    "    y = y.tolist()\n",
    "    return dset.get_str(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "cb4ff49e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "d = device\n",
    "dset = dataset.Arithmetic(10)\n",
    "model = RetraNet(d, optim.Adam, dset.max_len, dset.num_tokens, dim=256, nhead=32).to(d)\n",
    "model.toggleActor()\n",
    "op = optim.Adam(model.parameters(), lr=3e-8)\n",
    "scheduler = optim.lr_scheduler.StepLR(op, step_size=10000, gamma=.99)\n",
    "crit = nn.CrossEntropyLoss(ignore_index=dset.TOKENS[\"<PAD>\"])\n",
    "#convert(\"1+1\", dset, model, d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "a1bc5fff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 100 Loss: 2.6867427909374237\n",
      "Iteration: 200 Loss: 2.660132176876068\n",
      "Iteration: 300 Loss: 2.6668092918396\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[35], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m#torch.autograd.set_detect_anomaly(True)\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcrit\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43md\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1000000\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscheduler\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[33], line 16\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(model, optim, crit, device, iterations, dset, batch_size, print_freq, scheduler)\u001b[0m\n\u001b[1;32m     14\u001b[0m loss \u001b[38;5;241m=\u001b[39m crit(model_out\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, model_out\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]), y_\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m))\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m(model\u001b[38;5;241m.\u001b[39mencoders[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mactor_train):\n\u001b[0;32m---> 16\u001b[0m     \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpropagateLoss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     17\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m     19\u001b[0m optim\u001b[38;5;241m.\u001b[39mstep()\n",
      "Cell \u001b[0;32mIn[30], line 45\u001b[0m, in \u001b[0;36mRetraNet.propagateLoss\u001b[0;34m(self, loss)\u001b[0m\n\u001b[1;32m     43\u001b[0m     enc\u001b[38;5;241m.\u001b[39mpropagateLoss(loss)\n\u001b[1;32m     44\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m dec \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdecoders:\n\u001b[0;32m---> 45\u001b[0m     \u001b[43mdec\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpropagateLoss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mloss\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[29], line 56\u001b[0m, in \u001b[0;36mRetraDecBlock.propagateLoss\u001b[0;34m(self, loss)\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mactor_train)\n\u001b[1;32m     55\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mactor_critic\u001b[38;5;241m.\u001b[39maddReward(loss)\n\u001b[0;32m---> 56\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mactor_critic\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mactor_optim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mGAMMA\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[28], line 56\u001b[0m, in \u001b[0;36mPolicyNetwork.train\u001b[0;34m(self, OPTIM, gamma)\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[38;5;66;03m##OPTIMIZE##\u001b[39;00m\n\u001b[1;32m     55\u001b[0m OPTIM\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m---> 56\u001b[0m \u001b[43mtotal_loss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mretain_graph\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     57\u001b[0m OPTIM\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     59\u001b[0m \u001b[38;5;66;03m##CLEAR MEMORY##\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/_tensor.py:488\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    478\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    479\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    480\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    481\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    486\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    487\u001b[0m     )\n\u001b[0;32m--> 488\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    489\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    490\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/autograd/__init__.py:197\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    192\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    194\u001b[0m \u001b[38;5;66;03m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    195\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    196\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 197\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    198\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    199\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#torch.autograd.set_detect_anomaly(True)\n",
    "train(model, op, crit, d, 1000000, dset, 1, 100, scheduler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff5f3d82",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.toggleActor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "146b74a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.encoders[0].actor_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0b3d9cd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
