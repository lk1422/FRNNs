{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6885fbe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.distributions import Categorical\n",
    "from collections import namedtuple\n",
    "from torch import optim\n",
    "import numpy as np\n",
    "import numpy as np\n",
    "import math\n",
    "import gym\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e3215e55",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PolicyNetwork(nn.Module):\n",
    "    def __init__(self, state_space, n_actions):\n",
    "        super(PolicyNetwork, self).__init__()\n",
    "        self.base = nn.Linear(state_space, 256)\n",
    "        self.actions = nn.Linear(256, n_actions)\n",
    "        self.value = nn.Linear(256, 1)\n",
    "        self.rewards = []\n",
    "        self.action_pairs = [] #[(LOG_PROB, CRITIC_VALUE)]\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.base(x))\n",
    "        a = F.softmax(self.actions(x), dim=-1)\n",
    "        v = self.value(x)\n",
    "        return a,v\n",
    "    def select_action(self, state):\n",
    "        a, v = self.forward(state)\n",
    "        m = Categorical(a)\n",
    "        action = m.sample()\n",
    "        self.action_pairs.append((m.log_prob(action), v))\n",
    "        return action.item()\n",
    "    def train(self, OPTIM, gamma):\n",
    "        ##CONSTRUCT SAMPLED VALUES##\n",
    "        rewards = []\n",
    "        R = 0\n",
    "        for r in self.rewards[::-1]:\n",
    "            R = r + gamma * R\n",
    "            rewards.insert(0, R)\n",
    "        ##NORMALIZE SAMPLED STATE VALUES##\n",
    "        rewards = torch.tensor(rewards, requires_grad=False)\n",
    "        rewards = (rewards-rewards.mean())/(rewards.std() + 1e-4)\n",
    "        ##GET ACTOR AND CRITIC LOSS##\n",
    "        actor_loss = torch.tensor([0], dtype=torch.float32)\n",
    "        critic_loss = torch.tensor([0], dtype=torch.float32)\n",
    "        for (log_prob, val), R in zip(self.action_pairs, rewards):\n",
    "            advantage = R.item() - val.item()\n",
    "            actor_loss += -log_prob*advantage\n",
    "            critic_loss += F.smooth_l1_loss(val, torch.tensor([[R]]))\n",
    "        total_loss = actor_loss + critic_loss\n",
    "        ##OPTIMIZE##\n",
    "        OPTIM.zero_grad()\n",
    "        total_loss.backward()\n",
    "        OPTIM.step()\n",
    "        ##CLEAR MEMORY##\n",
    "        del self.rewards[:]\n",
    "        del self.action_pairs[:]\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "88226d79",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ac = PolicyNetwork(4, 2)\n",
    "ac.select_action(torch.randn(1,4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "08f302ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 1000\n",
    "ac = PolicyNetwork(4,2)\n",
    "env = gym.make(\"CartPole-v1\", render_mode=\"human\")\n",
    "OPTIM = optim.Adam(ac.parameters(), 3e-3)\n",
    "GAMMA = 0.99"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36eb3fc7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lk3ond/.local/lib/python3.10/site-packages/gym/utils/passive_env_checker.py:233: DeprecationWarning: `np.bool8` is a deprecated alias for `np.bool_`.  (Deprecated NumPy 1.24)\n",
      "  if not isinstance(terminated, (bool, np.bool8)):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Epoch total moving average 5.2\n"
     ]
    }
   ],
   "source": [
    "TRAIN_REWARDS = []\n",
    "moving_average = 0\n",
    "solved = False\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    total_reward = 0\n",
    "    state = env.reset()\n",
    "    state = torch.Tensor(state[0]).unsqueeze(0)\n",
    "    while (True):\n",
    "        action = ac.select_action(state)\n",
    "\n",
    "        next_state, reward, done, _, _ = env.step(action)\n",
    "        next_state = torch.Tensor(next_state)\n",
    "        ac.rewards.append(reward)\n",
    "        total_reward += reward\n",
    "\n",
    "        state = next_state.unsqueeze(0)\n",
    "        if(total_reward > 999):\n",
    "            print(\"SOLVED\")\n",
    "            TRAIN_REWARDS.append(total_reward)\n",
    "            solved = True\n",
    "            break\n",
    "        \n",
    "        if done:\n",
    "            ac.train(OPTIM, GAMMA)\n",
    "            TRAIN_REWARDS.append(total_reward)\n",
    "            moving_average = moving_average*(0.8) + (0.2)*total_reward\n",
    "            if(epoch % 100 == 0):\n",
    "                print(f\"{epoch} Epoch total moving average {moving_average}\")\n",
    "                \n",
    "            break\n",
    "    if solved:\n",
    "        break\n",
    "\n",
    "env.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4ccf9cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot(TRAIN_REWARDS)\n",
    "plt.ylabel('rewards')\n",
    "plt.xlabel('episodes')\n",
    "plt.title(\"Results of Actor Critic\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9d67e96",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
